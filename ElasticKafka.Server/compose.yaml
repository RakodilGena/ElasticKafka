volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
  elastic-1-data:
  elastic-2-data:
  elastic-3-data:

services:
  
  #Important node: minimum THREE elastic nodes required for successful master-election-quorum
  #if master dies somehow
  elastic-1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.2 #latest doesn't work
    container_name: elastic-1
    ports:
      - "9201:9200"
    environment:
      - bootstrap.memory_lock=true #prevents swapping memory to disk
      - xpack.security.enabled=false #disabled for dev env

      - node.name=elastic-1
      - cluster.name=elastic-cluster-storage
      
      #other nodes
      - discovery.seed_hosts=elastic-2,elastic-3
      
      #IMPORTANT: remove after cluster is formed
      #- cluster.initial_master_nodes=elastic-1,elastic-2,elastic-3

      #storage initial - max (512mb). inc for prod (eg -Xms2g -Xmx2g for 2 gb)
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m -Dlog4j2.formatMsgNoLookups=true"
    volumes:
      - elastic-1-data:/usr/share/elasticsearch/data
    restart: on-failure

  elastic-2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.2
    container_name: elastic-2
    ports:
      - "9202:9200"
    environment:
      - bootstrap.memory_lock=true #prevents swapping memory to disk
      - xpack.security.enabled=false #disabled for dev env

      - node.name=elastic-2
      - cluster.name=elastic-cluster-storage
      
      #other nodes
      - discovery.seed_hosts=elastic-1,elastic-3
      
      #IMPORTANT: remove after cluster is formed
      #- cluster.initial_master_nodes=elastic-1,elastic-2,elastic-3

      #storage initial - max (512mb). inc for prod (eg -Xms2g -Xmx2g for 2 gb)
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m -Dlog4j2.formatMsgNoLookups=true"
    volumes:
      - elastic-2-data:/usr/share/elasticsearch/data
    restart: on-failure
  
  elastic-3:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.2
    container_name: elastic-3
    ports:
      - "9203:9200"
    environment:
      - bootstrap.memory_lock=true #prevents swapping memory to disk
      - xpack.security.enabled=false #disabled for dev env

      - node.name=elastic-3
      - cluster.name=elastic-cluster-storage

      #other nodes
      - discovery.seed_hosts=elastic-1,elastic-2

      #IMPORTANT: remove after cluster is formed
      #- cluster.initial_master_nodes=elastic-1,elastic-2,elastic-3

      #storage initial - max (512mb). inc for prod (eg -Xms2g -Xmx2g for 2 gb)
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m -Dlog4j2.formatMsgNoLookups=true"
    volumes:
      - elastic-3-data:/usr/share/elasticsearch/data
    restart: on-failure
  
  
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    restart: on-failure
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
  
  
  
  kafka-1:
    image: confluentinc/cp-kafka:latest
    restart: on-failure
    ports:
      - '9091:9091'
      - '29091:29091'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      
      #internal to access from docker cluster, local - to access from elsewhere (from localhost apps too)
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_LISTENERS: LISTENER_INTERNAL://:9091,LISTENER_LOCAL://:29091
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-1:9091,LISTENER_LOCAL://localhost:29091
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_LOCAL:PLAINTEXT
      
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    volumes:
      - kafka-1-data:/var/lib/kafka1/data
    depends_on:
      - zookeeper

  kafka-2:
    image: confluentinc/cp-kafka:latest
    restart: on-failure
    ports:
      - "9092:9092"
      - '29092:29092'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_LISTENERS: LISTENER_INTERNAL://:9092,LISTENER_LOCAL://:29092
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-2:9092,LISTENER_LOCAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_LOCAL:PLAINTEXT

      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    volumes:
      - kafka-2-data:/var/lib/kafka2/data
    depends_on:
      - zookeeper

  kafka-3:
    image: confluentinc/cp-kafka:latest
    restart: on-failure
    ports:
      - "9093:9093"
      - '29093:29093'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_INTERNAL
      KAFKA_LISTENERS: LISTENER_INTERNAL://:9093,LISTENER_LOCAL://:29093
      KAFKA_ADVERTISED_LISTENERS: LISTENER_INTERNAL://kafka-3:9093,LISTENER_LOCAL://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_INTERNAL:PLAINTEXT,LISTENER_LOCAL:PLAINTEXT

      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    volumes:
      - kafka-3-data:/var/lib/kafka3/data
    depends_on:
      - zookeeper
  
  
  
  #  automatically creates requested kafka topics
  init-topics:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      echo -e 'Existing topics:'
      kafka-topics --bootstrap-server kafka-1:9091 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka-1:9091 --create --if-not-exists --topic NewMessages --replication-factor 3 --partitions 3
      kafka-topics --bootstrap-server kafka-1:9091 --create --if-not-exists --topic MessageCreatedEvents --replication-factor 3 --partitions 2

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka-1:9091 --list
      "
  
  
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    restart: on-failure
  
  
  gateway-service-1:
    image: gatewayservice
    restart: on-failure
    build:
      context: .
      dockerfile: GatewayService/Dockerfile
    ports:
      - "5101:8080"
    environment:
      #without HTTP, important!
      "ServiceDiscoveryUrls:Value": "service-discovery-1:8080,service-discovery-2:8080"
      
      "Redis:ConnectionString": "redis:6379"
      
      "Kafka:Consumers:MessageCreatedEvents:Topic": "MessageCreatedEvents"
      "Kafka:Consumers:MessageCreatedEvents:Config:GroupId": "gw_service_message_created_events"
      "Kafka:Consumers:MessageCreatedEvents:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Consumers:MessageCreatedEvents:Config:EnableAutoCommit": "false"
      
      "AllowOrigins": "http://localhost"
    depends_on:
      - redis
      - kafka-1
      - kafka-2
      - kafka-3
      - service-discovery-1
      - service-discovery-2
  
  gateway-service-2:
    image: gatewayservice
    restart: on-failure
    build:
      context: .
      dockerfile: GatewayService/Dockerfile
    ports:
      - "5102:8080"
    environment:
      #without HTTP, important!
      "ServiceDiscoveryUrls:Value": "service-discovery-2:8080,service-discovery-1:8080"

      "Redis:ConnectionString": "redis:6379"

      "Kafka:Consumers:MessageCreatedEvents:Topic": "MessageCreatedEvents"
      "Kafka:Consumers:MessageCreatedEvents:Config:GroupId": "gw_service_message_created_events"
      "Kafka:Consumers:MessageCreatedEvents:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Consumers:MessageCreatedEvents:Config:EnableAutoCommit": "false"
      
      "AllowOrigins": "http://localhost"
    depends_on:
      - redis
      - kafka-1
      - kafka-2
      - kafka-3
      - service-discovery-1
      - service-discovery-2
  
  messaging-service-1:
    image: messagingservice
    restart: on-failure
    build:
      context: .
      dockerfile: MessagingService/Dockerfile
    environment:
      "Kafka:Producer:NewMessagesTopic": "NewMessages"
      "Kafka:Producer:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Producer:Config:MessageTimeoutMs": "10000"
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
  
  messaging-service-2:
    image: messagingservice
    restart: on-failure
    build:
      context: .
      dockerfile: MessagingService/Dockerfile
    environment:
      "Kafka:Producer:NewMessagesTopic": "NewMessages"
      "Kafka:Producer:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Producer:Config:MessageTimeoutMs": "10000"
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
  
  
  messaging-service-3:
    image: messagingservice
    restart: on-failure
    build:
      context: .
      dockerfile: MessagingService/Dockerfile
    environment:
      "Kafka:Producer:NewMessagesTopic": "NewMessages"
      "Kafka:Producer:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Producer:Config:MessageTimeoutMs": "10000"
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
  
  
  
  storage-service-1:
    image: storageservice
    restart: on-failure
    build:
      context: .
      dockerfile: StorageService/Dockerfile
    environment:
      "Kafka:Producer:MessageCreatedEventsTopic": "MessageCreatedEvents"
      "Kafka:Producer:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Producer:Config:MessageTimeoutMs": "10000"
      
      "Kafka:Consumers:NewMessages:Topic": "NewMessages"
      "Kafka:Consumers:NewMessages:Config:GroupId": "messaging_service_new_messages"
      "Kafka:Consumers:NewMessages:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Consumers:NewMessages:Config:EnableAutoCommit": "false"
      
      "Elastic:Nodes": "http://elastic-1:9200,http://elastic-2:9200,http://elastic-3:9200"
    depends_on:
      - redis
      - kafka-1
      - kafka-2
      - kafka-3
      - elastic-1
      - elastic-2
      - elastic-3
      - storage-service-migrator
  
  storage-service-2:
    image: storageservice
    restart: on-failure
    build:
      context: .
      dockerfile: StorageService/Dockerfile
    environment:
      "Kafka:Producer:MessageCreatedEventsTopic": "MessageCreatedEvents"
      "Kafka:Producer:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Producer:Config:MessageTimeoutMs": "10000"
      
      "Kafka:Consumers:NewMessages:Topic": "NewMessages"
      "Kafka:Consumers:NewMessages:Config:GroupId": "messaging_service_new_messages"
      "Kafka:Consumers:NewMessages:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Consumers:NewMessages:Config:EnableAutoCommit": "false"
      
      "Elastic:Nodes": "http://elastic-1:9200,http://elastic-2:9200,http://elastic-3:9200"
    depends_on:
      - redis
      - kafka-1
      - kafka-2
      - kafka-3
      - elastic-1
      - elastic-2
      - elastic-3
      - storage-service-migrator
  
  
  storage-service-3:
    image: storageservice
    restart: on-failure
    build:
      context: .
      dockerfile: StorageService/Dockerfile
    environment:
      "Kafka:Producer:MessageCreatedEventsTopic": "MessageCreatedEvents"
      "Kafka:Producer:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Producer:Config:MessageTimeoutMs": "10000"
      
      "Kafka:Consumers:NewMessages:Topic": "NewMessages"
      "Kafka:Consumers:NewMessages:Config:GroupId": "messaging_service_new_messages"
      "Kafka:Consumers:NewMessages:Config:BootstrapServers": "kafka-1:9091,kafka-2:9092,kafka-3:9093"
      "Kafka:Consumers:NewMessages:Config:EnableAutoCommit": "false"
      
      "Elastic:Nodes": "http://elastic-1:9200,http://elastic-2:9200,http://elastic-3:9200"
    depends_on:
      - redis
      - kafka-1
      - kafka-2
      - kafka-3
      - elastic-1
      - elastic-2
      - elastic-3
      - storage-service-migrator
  
  storage-service-migrator:
    image: storageservice
    restart: on-failure #will actually restart until elastic reached and index created / exists checked
    build:
      context: .
      dockerfile: StorageService/Dockerfile
    environment:
      "MIGRATE": "true"
      "Elastic:Nodes": "http://elastic-1:9200,http://elastic-2:9200,http://elastic-3:9200"
    depends_on:
      - elastic-1
      - elastic-2
      - elastic-3
  
  service-discovery-1:
    image: servicediscovery
    restart: on-failure
    build:
      context: .
      dockerfile: ServiceDiscovery/Dockerfile
    environment:
      "ServiceUrls:MessagingServices": "http://messaging-service-1:8080,http://messaging-service-2:8080,http://messaging-service-3:8080"
      "ServiceUrls:StorageServices": "http://storage-service-1:8080,http://storage-service-2:8080,http://storage-service-3:8080"

  service-discovery-2:
    image: servicediscovery
    restart: on-failure
    build:
      context: .
      dockerfile: ServiceDiscovery/Dockerfile
    environment:
      "ServiceUrls:MessagingServices": "http://messaging-service-1:8080,http://messaging-service-2:8080,http://messaging-service-3:8080"
      "ServiceUrls:StorageServices": "http://storage-service-1:8080,http://storage-service-2:8080,http://storage-service-3:8080"

